{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "All_features_chinese_finalversion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC6fcXiSTiSL"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "import copy\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgHDxZPhg73d"
      },
      "source": [
        "# Global Vocabs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ2dL6lDZQCl"
      },
      "source": [
        "english_dev = 'en_ewt-ud-train-projectivized.conllu'\r\n",
        "english_train = 'en_ewt-ud-dev.conllu'\r\n",
        "chinese_dev = 'zh_gsdsimp-ud-dev.conllu'\r\n",
        "chinese_train = 'zh_gsdsimp-ud-train.conllu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23MicKUg61G"
      },
      "source": [
        "class LabelDataset():\r\n",
        "\r\n",
        "    ROOT = ('<root>', '<root>', 0, '<root>')  # Pseudo-root\r\n",
        "\r\n",
        "    def __init__(self, filename):\r\n",
        "        self.filename = filename\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        with open(self.filename, 'rt', encoding='utf-8') as lines:\r\n",
        "            tmp = [LabelDataset.ROOT]\r\n",
        "            for line in lines:\r\n",
        "                if not line.startswith('#'):  # Skip lines with comments\r\n",
        "                    line = line.rstrip()\r\n",
        "                    if line:\r\n",
        "                        columns = line.split('\\t')\r\n",
        "                        if columns[0].isdigit():  # Skip range tokens\r\n",
        "                            tmp.append((columns[1], columns[3], int(columns[6]), columns[7])) #Colums 7 is dependency label\r\n",
        "                    else:\r\n",
        "                        yield tmp\r\n",
        "                        tmp = [LabelDataset.ROOT]\r\n",
        "\r\n",
        "label_train_data = LabelDataset(chinese_train)\r\n",
        "label_dev_data = LabelDataset(chinese_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkdVdzAshTgk"
      },
      "source": [
        "PAD = '<pad>'\r\n",
        "UNK = '<unk>'\r\n",
        "ROOT = '<root>'\r\n",
        "def make_vocabs(label_gold_data):\r\n",
        "    # TODO: Replace the next line with your own code\r\n",
        "    word_vocab = {PAD:0,UNK:1}\r\n",
        "    tag_vocab = {PAD:0}\r\n",
        "    label_vocab = {PAD:0, ROOT:1}\r\n",
        "    inverse_word_vocab = {0:PAD,1:UNK}\r\n",
        "    inverse_tag_vocab = {0:PAD}\r\n",
        "    inverse_label_vocab = {0:PAD, 1:ROOT}\r\n",
        "    word_i = 2\r\n",
        "    tag_i = 1\r\n",
        "    label_i = 2\r\n",
        "    for sentence in label_gold_data:\r\n",
        "        for pair in sentence:\r\n",
        "            if not (pair[0] in word_vocab):\r\n",
        "                word_vocab[pair[0]] = word_i\r\n",
        "                inverse_word_vocab[word_i] = pair[0]\r\n",
        "                word_i += 1\r\n",
        "            if not (pair[1] in tag_vocab):\r\n",
        "                tag_vocab[pair[1]] = tag_i\r\n",
        "                inverse_tag_vocab[tag_i] = pair[1]\r\n",
        "                tag_i += 1\r\n",
        "            if not (pair[3] in label_vocab):\r\n",
        "                label_vocab[pair[3]] = label_i\r\n",
        "                inverse_label_vocab[label_i] = pair[3]\r\n",
        "                label_i += 1           \r\n",
        "    return word_vocab, tag_vocab, label_vocab, inverse_word_vocab, inverse_tag_vocab, inverse_label_vocab\r\n",
        "\r\n",
        "vocab_words, vocab_tags, vocab_labels, inverse_vocab_words, inverse_vocab_tags, inverse_label_vocab = make_vocabs(label_train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS19zIMOXgYo"
      },
      "source": [
        "# Tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1YILdVbXikD"
      },
      "source": [
        "class tag_Dataset():\r\n",
        "\r\n",
        "    def __init__(self, filename):\r\n",
        "        self.filename = filename\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        with open(self.filename, 'rt', encoding='utf-8') as lines:\r\n",
        "            tmp = []\r\n",
        "            for line in lines:\r\n",
        "                if not line.startswith('#'):  # Skip lines with comments\r\n",
        "                    line = line.rstrip()\r\n",
        "                    if line:\r\n",
        "                        columns = line.split('\\t')\r\n",
        "                        if columns[0].isdigit():  # Skip range tokens\r\n",
        "                            tmp.append((columns[1], columns[3])) \r\n",
        "                    else:\r\n",
        "                        yield tmp\r\n",
        "                        tmp = []\r\n",
        "\r\n",
        "tag_train_data = tag_Dataset(chinese_train)\r\n",
        "tag_dev_data = tag_Dataset(chinese_dev)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkp6mOQ3YjO-"
      },
      "source": [
        "def accuracy(tagger, gold_data):\r\n",
        "    tp = 0\r\n",
        "    total = 0\r\n",
        "    for sentence in gold_data:\r\n",
        "        tokens = []\r\n",
        "        tags = []\r\n",
        "        for pair in sentence:\r\n",
        "            tokens.append(pair[0])\r\n",
        "            tags.append(pair[1])\r\n",
        "        for i, pred_tag in enumerate(tagger.predict(tokens)):\r\n",
        "            if sentence[i][1] == pred_tag:\r\n",
        "                tp += 1\r\n",
        "            total += 1\r\n",
        "    return tp/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIgeM6quYyVd"
      },
      "source": [
        "class tag_FixedWindowModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, embedding_specs = [(3, len(vocab_words), 50), (1, len(vocab_tags), 10)],\r\n",
        "                 hidden_dim = 100, output_dim = len(vocab_tags)):\r\n",
        "        super().__init__()\r\n",
        "        embed_out_dim = 0\r\n",
        "        embed_list = []\r\n",
        "        n_cols = 0\r\n",
        "        iters = []\r\n",
        "        embed_i = 0\r\n",
        "        for n, num_words, word_dim in embedding_specs:\r\n",
        "            tmp_embedding_layer = nn.Embedding(num_words, word_dim)\r\n",
        "            nn.init.normal_(tmp_embedding_layer.weight, std=0.01)\r\n",
        "            embed_list.append(tmp_embedding_layer)\r\n",
        "            embed_out_dim += word_dim * n\r\n",
        "            iters += [embed_i] * n\r\n",
        "            embed_i += 1\r\n",
        "            n_cols += n\r\n",
        "        self.iters = iters\r\n",
        "        self.n_cols = n_cols\r\n",
        "        self.embed = nn.ModuleList(embed_list)\r\n",
        "        self.linear1 = nn.Linear(embed_out_dim, hidden_dim)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\r\n",
        "\r\n",
        "    def forward(self, features):\r\n",
        "        if len(features.shape) == 1:\r\n",
        "            features = torch.unsqueeze(features, dim=0)\r\n",
        "        features = torch.cat(tuple(self.embed[self.iters[i]](features[:,i]) for i in range(self.n_cols)), 1)\r\n",
        "        features = self.linear1(features)\r\n",
        "        features = self.relu(features)\r\n",
        "        features = self.linear2(features)\r\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qVJ47MxY6wl"
      },
      "source": [
        "class Tagger(object):\r\n",
        "\r\n",
        "    def predict(self, sentence):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "class FixedWindowTagger(Tagger):\r\n",
        "\r\n",
        "    def __init__(self, vocab_words, vocab_tags, inverse_vocab_words, inverse_vocab_tags, output_dim = 4, word_dim=50, tag_dim=10, hidden_dim=100):\r\n",
        "        self.fw_model = tag_FixedWindowModel()\r\n",
        "        self.vocab_words = vocab_words\r\n",
        "        self.vocab_tags = vocab_tags\r\n",
        "        self.inverse_vocab_words = inverse_vocab_words\r\n",
        "        self.inverse_vocab_tags = inverse_vocab_tags\r\n",
        "\r\n",
        "    def featurize(self, words, i, pred_tags):\r\n",
        "        tagger_conf = [0] * 4\r\n",
        "        tagger_conf[0] = words[i]\r\n",
        "        if i-1 < 0:\r\n",
        "            tagger_conf[1] = self.vocab_words[PAD]\r\n",
        "            tagger_conf[3] = self.vocab_tags[PAD]\r\n",
        "        else:\r\n",
        "            tagger_conf[1] = words[i-1]\r\n",
        "            tagger_conf[3] = pred_tags[i-1]\r\n",
        "        if i+1 > len(words)-1:\r\n",
        "            tagger_conf[2] = self.vocab_words[PAD]\r\n",
        "        else:\r\n",
        "            tagger_conf[2] = words[i+1]\r\n",
        "        return torch.LongTensor(tagger_conf)\r\n",
        "\r\n",
        "    def predict(self, words):\r\n",
        "        pred_tag_ids = []\r\n",
        "        word_ids = []\r\n",
        "        for word in words:\r\n",
        "\r\n",
        "            word_ids.append(vocab_words[word] if word in vocab_words else vocab_words[UNK])\r\n",
        "\r\n",
        "        for i in range(len(word_ids)):\r\n",
        "            window = self.featurize(word_ids, i, pred_tag_ids)\r\n",
        "            pred = self.fw_model.forward(window)\r\n",
        "            pred_tag_ids.append(torch.argmax(pred).item())\r\n",
        "        pred_tags = []\r\n",
        "        for tag_id in pred_tag_ids:\r\n",
        "            pred_tags.append(inverse_vocab_tags[tag_id])\r\n",
        "\r\n",
        "        return pred_tags\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2og4VB8WZGBX"
      },
      "source": [
        "def tag_training_examples(vocab_words, vocab_tags, inverse_vocab_words, inverse_vocab_tags, gold_data, tagger, batch_size=100):\r\n",
        "  batch_i = 0\r\n",
        "  batch_x = torch.zeros((batch_size, 4), dtype=int)\r\n",
        "  batch_y = torch.zeros(batch_size, dtype=int)\r\n",
        "  for sentence in gold_data:\r\n",
        "      word_ids = []\r\n",
        "      tag_ids = []\r\n",
        "      for pair in sentence:\r\n",
        "          word_ids.append(vocab_words[pair[0]])\r\n",
        "          tag_ids.append(vocab_tags[pair[1]])\r\n",
        "      for i in range(len(word_ids)):\r\n",
        "          if batch_i > batch_size - 1: \r\n",
        "              yield batch_x, batch_y\r\n",
        "              batch_x = torch.zeros((batch_size, 4), dtype=int)\r\n",
        "              batch_y = torch.zeros(batch_size, dtype=int)\r\n",
        "              batch_i = 0        \r\n",
        "          batch_x[batch_i,:] = tagger.featurize(word_ids, i, tag_ids)\r\n",
        "          batch_y[batch_i] = tag_ids[i]\r\n",
        "          batch_i += 1\r\n",
        "  if not batch_i == 0:\r\n",
        "      yield batch_x[0:batch_i,:], batch_y[0:batch_i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F3tPGCqZHIc"
      },
      "source": [
        "def tag_train_fixed_window(train_data, n_epochs=2, batch_size=100, lr=1e-2):\r\n",
        "    vocab_words, vocab_tags, inverse_vocab_words, inverse_vocab_tags,_,_ = make_vocabs(label_train_data)\r\n",
        "    fw_tagger = FixedWindowTagger(vocab_words, vocab_tags, inverse_vocab_words, inverse_vocab_tags)\r\n",
        "    optimizer = optim.Adam(fw_tagger.fw_model.parameters(), lr=lr)\r\n",
        "\r\n",
        "    for ep in range(n_epochs):\r\n",
        "        fw_tagger.fw_model.train()\r\n",
        "        loss_sum = 0\r\n",
        "        n_batch = 0\r\n",
        "        for bx, by in tag_training_examples(vocab_words, vocab_tags, inverse_vocab_words, inverse_vocab_tags, train_data, fw_tagger, batch_size=batch_size):\r\n",
        "            bx = bx.to(device)\r\n",
        "            by = by.to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            output = fw_tagger.fw_model.forward(bx)\r\n",
        "            loss = F.cross_entropy(output, by)\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            loss_sum += loss.item()\r\n",
        "            n_batch += 1      \r\n",
        "        fw_tagger.fw_model.eval()\r\n",
        "        #print(\"Accuracy on validation data: \" + str(accuracy(fw_tagger, tag_dev_data)))\r\n",
        "    return fw_tagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxbxNec0LP9Q"
      },
      "source": [
        "# Labeller"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSn6Qn57hHnO"
      },
      "source": [
        "Data and vocabs already created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DeHF6uSLZxZ"
      },
      "source": [
        "class label_FixedWindowModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, embedding_specs = [(2, len(vocab_words), 50), (2, len(vocab_tags), 10)],\r\n",
        "                 hidden_dim = 180, output_dim = len(vocab_labels)):\r\n",
        "        super().__init__()\r\n",
        "        embed_out_dim = 0\r\n",
        "        embed_list = []\r\n",
        "        n_cols = 0\r\n",
        "        iters = []\r\n",
        "        embed_i = 0\r\n",
        "        for n, num_words, word_dim in embedding_specs:\r\n",
        "            tmp_embedding_layer = nn.Embedding(num_words, word_dim)\r\n",
        "            nn.init.normal_(tmp_embedding_layer.weight, std=0.01)\r\n",
        "            embed_list.append(tmp_embedding_layer)\r\n",
        "            embed_out_dim += word_dim * n\r\n",
        "            iters += [embed_i] * n\r\n",
        "            embed_i += 1\r\n",
        "            n_cols += n\r\n",
        "        self.iters = iters\r\n",
        "        self.n_cols = n_cols\r\n",
        "        self.embed = nn.ModuleList(embed_list)\r\n",
        "        self.linear1 = nn.Linear(embed_out_dim, hidden_dim)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\r\n",
        "\r\n",
        "    def forward(self, features):\r\n",
        "        if len(features.shape) == 1:\r\n",
        "            features = torch.unsqueeze(features, dim=0)\r\n",
        "        features = torch.cat(tuple(self.embed[self.iters[i]](features[:,i]) for i in range(self.n_cols)), 1)\r\n",
        "        features = self.linear1(features)\r\n",
        "        features = self.relu(features)\r\n",
        "        features = self.linear2(features)\r\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd94mUkELcav"
      },
      "source": [
        "class FixedWindowLabler():\r\n",
        "\r\n",
        "    def __init__(self, vocab_words, vocab_tags, inverse_vocab_labels, hidden_dim=180):\r\n",
        "        super().__init__()\r\n",
        "        self.fw_model = label_FixedWindowModel()\r\n",
        "        self.vocab_words = vocab_words\r\n",
        "        self.vocab_tags = vocab_tags\r\n",
        "        self.inverse_vocab_labels = inverse_vocab_labels\r\n",
        " \r\n",
        "\r\n",
        "    def featurize(self, words, tags, indexes): #config = (source_i, target_i)\r\n",
        "        features = [0]*4\r\n",
        "        source_i, target_i = indexes\r\n",
        "        \r\n",
        "        if source_i:\r\n",
        "            features[0] = words[source_i]\r\n",
        "            features[2] = tags[source_i]\r\n",
        "        else:\r\n",
        "            features[0] = self.vocab_tags[PAD]\r\n",
        "            features[2] = self.vocab_tags[PAD]         \r\n",
        "        if target_i:\r\n",
        "            features[1] = words[target_i]\r\n",
        "            features[3] = tags[target_i]\r\n",
        "        else:\r\n",
        "            features[1] = self.vocab_tags[PAD]\r\n",
        "            features[3] = self.vocab_tags[PAD]  \r\n",
        "        return torch.LongTensor(features)\r\n",
        "\r\n",
        "\r\n",
        "    def predict(self, source_word_id, target_word_id, source_tag_id, target_tag_id, want_print = False):\r\n",
        "        window = torch.LongTensor([source_word_id, target_word_id, source_tag_id, target_tag_id])\r\n",
        "        pred = self.fw_model.forward(window)\r\n",
        "        pred_label_id = torch.argmax(pred).item()\r\n",
        "        return pred_label_id\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8LAzbYgLfV-"
      },
      "source": [
        "def label_training_examples(vocab_words, vocab_tags, vocab_labels, gold_data, labler, batch_size=100):\r\n",
        "  batch_i = 0\r\n",
        "  K = 4\r\n",
        "  batch_x = torch.zeros((batch_size, K), dtype=int)\r\n",
        "  batch_y = torch.zeros(batch_size, dtype=int)\r\n",
        "  for sentence in gold_data:\r\n",
        "      word_ids = []\r\n",
        "      tag_ids = []\r\n",
        "      heads = []\r\n",
        "      label_ids = []\r\n",
        "      for quad in sentence:\r\n",
        "          word_ids.append(vocab_words[quad[0]])\r\n",
        "          tag_ids.append(vocab_tags[quad[1]])\r\n",
        "          heads.append(quad[2])\r\n",
        "          label_ids.append(vocab_labels[quad[3]])\r\n",
        "      \r\n",
        "      for i in range(len(word_ids)):\r\n",
        "          if batch_i > batch_size - 1: \r\n",
        "              yield batch_x, batch_y\r\n",
        "              batch_x = torch.zeros((batch_size, 4), dtype=int)\r\n",
        "              batch_y = torch.zeros(batch_size, dtype=int)\r\n",
        "              batch_i = 0        \r\n",
        "          batch_x[batch_i,:] = labler.featurize(word_ids, tag_ids, (i, heads[i]))\r\n",
        "          batch_y[batch_i] = label_ids[i]\r\n",
        "          batch_i += 1\r\n",
        "  if not batch_i == 0:\r\n",
        "      yield batch_x[0:batch_i,:], batch_y[0:batch_i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSf0El5ULjTX"
      },
      "source": [
        "def label_train_fixed_window(train_data, n_epochs=2, batch_size=100, lr=1e-2):\r\n",
        "    vocab_words, vocab_tags, vocab_labels, inverse_vocab_words, inverse_vocab_tags, inverse_vocab_labels = make_vocabs(label_train_data)\r\n",
        "    labler = FixedWindowLabler(vocab_words, vocab_tags, inverse_vocab_labels)\r\n",
        "    optimizer = optim.Adam(labler.fw_model.parameters(), lr=lr)\r\n",
        "    \r\n",
        "    for ep in range(n_epochs):\r\n",
        "        labler.fw_model.train()\r\n",
        "        n_batch = 0\r\n",
        "        for bx, by in label_training_examples(vocab_words, vocab_tags, vocab_labels, train_data, labler, batch_size=batch_size):\r\n",
        "            bx = bx.to(device)\r\n",
        "            by = by.to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            output = labler.fw_model.forward(bx)\r\n",
        "            loss = 0\r\n",
        "            try:\r\n",
        "                loss = F.cross_entropy(output, by)\r\n",
        "            except:\r\n",
        "                print(by.shape)\r\n",
        "                print(output.shape)\r\n",
        "                print(vocab_labels)\r\n",
        "                print(vocab_words)\r\n",
        "                print(vocab_tags)\r\n",
        "                for elem in by:\r\n",
        "                    try:\r\n",
        "                        inverse_vocab_labels[elem.item()]\r\n",
        "                    except:\r\n",
        "                        print(\"by\")\r\n",
        "                        print(elem.item())\r\n",
        "                \r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            n_batch += 1\r\n",
        "        labler.fw_model.eval()\r\n",
        "    \r\n",
        "    return labler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aRFpzPThles"
      },
      "source": [
        "labler = label_train_fixed_window(label_train_data, n_epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRXLdbhyTjnP"
      },
      "source": [
        "# Parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhs_UhL1Uh9l"
      },
      "source": [
        "class Dataset():\r\n",
        "\r\n",
        "    ROOT = ('<root>', '<root>', 0)  # Pseudo-root\r\n",
        "\r\n",
        "    def __init__(self, filename):\r\n",
        "        self.filename = filename\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        with open(self.filename, 'rt', encoding='utf-8') as lines:\r\n",
        "            tmp = [Dataset.ROOT]\r\n",
        "            for line in lines:\r\n",
        "                if not line.startswith('#'):  # Skip lines with comments\r\n",
        "                    line = line.rstrip()\r\n",
        "                    if line:\r\n",
        "                        columns = line.split('\\t')\r\n",
        "                        if columns[0].isdigit():  # Skip range tokens\r\n",
        "                            tmp.append((columns[1], columns[3], int(columns[6])))\r\n",
        "                    else:\r\n",
        "                        yield tmp\r\n",
        "                        tmp = [Dataset.ROOT]\r\n",
        "\r\n",
        "parse_train_data = Dataset(chinese_train)\r\n",
        "parse_dev_data = Dataset(chinese_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaZeRBPolnbA"
      },
      "source": [
        "def uas(parser, gold_data):\r\n",
        "    tp = 0\r\n",
        "    total = 0\r\n",
        "    for sentence in gold_data:\r\n",
        "        words, tags, heads = zip(*sentence)\r\n",
        "        pred = parser.predict(words, tags)\r\n",
        "        for i in range(1, len(pred)):\r\n",
        "            if heads[i] == pred[i]:\r\n",
        "                tp += 1\r\n",
        "            total += 1\r\n",
        "            #print(\"Iteration: \" + str(total) + \", UAS: \" + str(tp/total))\r\n",
        "    return tp/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3URwOrjjTnfw"
      },
      "source": [
        "def oracle_moves(gold_heads):\r\n",
        " \r\n",
        "    MOVES = tuple(range(3))\r\n",
        "    SH, LA, RA = MOVES\r\n",
        "\r\n",
        "    parser = ArcStandardParser()\r\n",
        "    config = parser.initial_config(len(gold_heads))\r\n",
        "  \r\n",
        "   \r\n",
        "    while (not parser.is_final_config(config)):\r\n",
        "        valid_moves = parser.valid_moves(config)\r\n",
        "\r\n",
        "        if len(valid_moves) >= 2:\r\n",
        "            indices1 = [i for i, x in enumerate(gold_heads) if x == config[1][-2]]\r\n",
        "            indices2 = [i for i, x in enumerate(config[2]) if x == config[1][-2]]\r\n",
        "            indices3 = [i for i, x in enumerate(gold_heads) if x == config[1][-1]]\r\n",
        "            indices4 = [i for i, x in enumerate(config[2]) if x == config[1][-1]]\r\n",
        "            if LA in valid_moves and gold_heads[config[1][-2]] == config[1][-1] and all(item in indices2 for item in indices1):       \r\n",
        "                yield (config, LA)\r\n",
        "                config = parser.next_config(config, LA)\r\n",
        "            elif ((RA in valid_moves) and (gold_heads[config[1][-1]] == config[1][-2]) and (all(item in indices4 for item in indices3))):\r\n",
        "                yield (config, RA)\r\n",
        "                config = parser.next_config(config, RA)\r\n",
        "            else:\r\n",
        "                yield (config, SH)\r\n",
        "                config = parser.next_config(config, SH)\r\n",
        "              \r\n",
        "        else:\r\n",
        "            yield (parser.next_config(config, SH), SH)\r\n",
        "            config = parser.next_config(config, SH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYNNXIecTzjo"
      },
      "source": [
        "class Parser(object):\r\n",
        "\r\n",
        "    def predict(self, words, tags):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "class ArcStandardParser(Parser):\r\n",
        "\r\n",
        "    MOVES = tuple(range(3))\r\n",
        "\r\n",
        "    SH, LA, RA = MOVES  # Parser moves are specified as integers.\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def initial_config(num_words):\r\n",
        "        return (0, [], [0]*num_words)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def valid_moves(config):\r\n",
        "        valid_moves = []\r\n",
        "        i, stack, head = config\r\n",
        "        if i < len(head):\r\n",
        "            valid_moves.append(0)\r\n",
        "        if len(stack) >= 2:\r\n",
        "            valid_moves.append(1)\r\n",
        "            valid_moves.append(2)\r\n",
        "        return valid_moves\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def next_config(config, move):\r\n",
        "        i, stack, head = config\r\n",
        "        i, stack, head = copy.deepcopy(i), copy.deepcopy(stack), copy.deepcopy(head)\r\n",
        "        if move == 0:\r\n",
        "            stack.append(i)\r\n",
        "            i += 1\r\n",
        "            return (i, stack, head)\r\n",
        "        if move == 1:\r\n",
        "            head[stack[-2]] = stack[-1]\r\n",
        "            del stack[-2]\r\n",
        "            return (i, stack, head)\r\n",
        "        if move == 2:\r\n",
        "            head[stack[-1]] = stack[-2]\r\n",
        "            del stack[-1]\r\n",
        "            return (i, stack, head)\r\n",
        "        print(\"Error,  config: \"+ str(config) + \"move: \" + str(move))\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def is_final_config(config):\r\n",
        "        i, stack, head = config\r\n",
        "\r\n",
        "        if i == len(head) and len(stack) == 1:\r\n",
        "            return True\r\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvpDr4ZfUfeF"
      },
      "source": [
        "class parse_FixedWindowModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, embedding_specs,\r\n",
        "                 hidden_dim = 180, output_dim = 3, \r\n",
        "                 use_distance = False, use_valency = False): \r\n",
        "        super().__init__()\r\n",
        "        self.n_word_features = 0\r\n",
        "        self.use_distance = use_distance\r\n",
        "        self.use_valency = use_valency\r\n",
        "        embed_out_dim = 0\r\n",
        "        embed_list = []\r\n",
        "        n_cols = 0\r\n",
        "        iters = []\r\n",
        "        embed_i = 0\r\n",
        "        for n, num_words, word_dim in embedding_specs:\r\n",
        "            tmp_embedding_layer = nn.Embedding(num_words, word_dim)\r\n",
        "            nn.init.normal_(tmp_embedding_layer.weight, std=0.01)\r\n",
        "            embed_list.append(tmp_embedding_layer)\r\n",
        "            embed_out_dim += word_dim * n\r\n",
        "            iters += [embed_i] * n \r\n",
        "            if embed_i == 0:\r\n",
        "                self.n_word_features = n\r\n",
        "                self.word_dim = word_dim\r\n",
        "            if embed_i == 1:\r\n",
        "                self.tag_dim = word_dim\r\n",
        "            embed_i += 1\r\n",
        "            n_cols += n\r\n",
        "        self.iters = iters\r\n",
        "        self.n_cols = n_cols\r\n",
        "        self.embed = nn.ModuleList(embed_list)\r\n",
        "        if self.use_distance:\r\n",
        "            #embed_out_dim += self.word_dim + self.tag_dim\r\n",
        "            embed_out_dim += 2\r\n",
        "        if self.use_valency:\r\n",
        "            embed_out_dim += 4 #Concatenate valencies to input tensor in linnear layer\r\n",
        "        self.linear1 = nn.Linear(embed_out_dim, hidden_dim)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\r\n",
        "\r\n",
        "    def forward(self, features, want_print = False):\r\n",
        "        if want_print:\r\n",
        "            print(\"In forward: \"+ str(features.shape))\r\n",
        "        if len(features.shape) == 1:\r\n",
        "            features = torch.unsqueeze(features, dim=0)\r\n",
        "        valencies = features[:,-5:-1] #Last 4 features\r\n",
        "        features = torch.cat(tuple(self.embed[self.iters[i]](features[:,i]) for i in range(self.n_cols)), 1)\r\n",
        "        ###Distance####\r\n",
        "        \"\"\"\r\n",
        "        if self.use_distance: \r\n",
        "            features = torch.cat((features, torch.abs(features[:,self.word_dim:2*self.word_dim] \r\n",
        "                                - features[:,2*self.word_dim:3*self.word_dim])), 1)\r\n",
        "            features = torch.cat((features, torch.abs(features[:,self.n_word_features*self.word_dim + self.tag_dim:\r\n",
        "                                               self.n_word_features*self.word_dim + 2*self.tag_dim] \r\n",
        "                                      - features[:,self.n_word_features*self.word_dim + 2*self.tag_dim:\r\n",
        "                                               self.n_word_features*self.word_dim + 3*self.tag_dim])), 1)\r\n",
        "        \"\"\"\r\n",
        "        if self.use_distance:\r\n",
        "            tmp_diff = torch.unsqueeze(features[:,self.word_dim:2*self.word_dim] \r\n",
        "                                      - features[:,2*self.word_dim:3*self.word_dim], 1)\r\n",
        "            features = torch.cat((features, torch.sum(torch.pow(tmp_diff, 2), 2)),1)\r\n",
        "            tmp_diff = torch.unsqueeze(features[:,self.n_word_features*self.word_dim + self.tag_dim:\r\n",
        "                                                  self.n_word_features*self.word_dim + 2*self.tag_dim] \r\n",
        "                                          - features[:,self.n_word_features*self.word_dim + 2*self.tag_dim:\r\n",
        "                                                  self.n_word_features*self.word_dim + 3*self.tag_dim], 1)\r\n",
        "            features = torch.cat((features, torch.sum(torch.pow(tmp_diff, 2), 2)),1)\r\n",
        "        ###Distance end####\r\n",
        "        if self.use_valency:\r\n",
        "            features = torch.cat((features, valencies), 1)\r\n",
        "        features = self.linear1(features)\r\n",
        "        features = self.relu(features)\r\n",
        "        features = self.linear2(features)\r\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LCXll3puw-a"
      },
      "source": [
        "class FixedWindowParser(ArcStandardParser):\r\n",
        "    def __init__(self, vocab_words, vocab_tags, embedding_specs, \r\n",
        "                 vocab_labels=vocab_labels, labler=labler, hidden_dim=180, \r\n",
        "                 use_distance = False, use_unigram = False, \r\n",
        "                 use_third_order = False, use_valency = False):\r\n",
        "        super().__init__()\r\n",
        "        self.fw_model = parse_FixedWindowModel(embedding_specs,\r\n",
        "                                               use_distance=use_distance,\r\n",
        "                                               use_valency=use_valency)\r\n",
        "        self.vocab_words = vocab_words\r\n",
        "        self.vocab_tags = vocab_tags\r\n",
        "        self.vocab_labels = vocab_labels\r\n",
        "        self.labler = labler\r\n",
        "        self.use_unigram = use_unigram\r\n",
        "        self.use_third_order = use_third_order\r\n",
        "        self.use_valency = use_valency\r\n",
        " \r\n",
        "\r\n",
        "    def featurize(self, words, tags, config, want_print = False):\r\n",
        "        word_features = []\r\n",
        "        tag_features = []\r\n",
        "        label_features = []\r\n",
        "        i, stack, head = config\r\n",
        "        i, stack, head = copy.deepcopy(i), copy.deepcopy(stack), copy.deepcopy(head)\r\n",
        "        #s0Lw s0Lp s0Rw s0Rp\r\n",
        "\r\n",
        "        #0. word form of the next word in the buffer\r\n",
        "        #1. word form of the topmost word on the stack\r\n",
        "        #2. word form of the second-topmost word on the stack\r\n",
        "        #3. word form of the leftmost modifier of topmost word on stack\r\n",
        "        #4. word form of the rightmost modifier of topmost word on stack\r\n",
        "        #5. word form of the leftmost modifier of second topmost word on stack\r\n",
        "        #6. word form of the rightmost modifier of second topmost word on stack (Maybe should be removed)\r\n",
        "        #7. part-of-speech tag of the next word in the buffer\r\n",
        "        #8. part-of-speech tag of the topmost word on the stack\r\n",
        "        #9. part-of-speech tag of the second-topmost word on the stack\r\n",
        "        #10. part-of-speech tag of the leftmost modifier of topmost word on stack\r\n",
        "        #11. part-of-speech tag of the rightmost modifier of topmost word on stack\r\n",
        "        #12. part-of-speech tag of the leftmost modifier of second topmost word on stack\r\n",
        "        #13. part-of-speech tag of the rightmost modifier of second topmost word on stack (Maybe should be removed)\r\n",
        "        #14. dependency label of the leftmost modifier of topmost word on stack\r\n",
        "        #15. dependency label of the rightmost modifier of topmost word on stack\r\n",
        "        #16. dependency label of the leftmost modifier of second topmost word on stack\r\n",
        "        #17. dependency label of the rightmost modifier of second topmost word on stack (Maybe should be removed)\r\n",
        "\r\n",
        "        #Feature 0, 7 (next word in buffer):\r\n",
        "        if i < len(words):\r\n",
        "            word_features.append(words[i])\r\n",
        "            tag_features.append(tags[i])  \r\n",
        "        else:\r\n",
        "            word_features.append(self.vocab_words[PAD])\r\n",
        "            tag_features.append(self.vocab_tags[PAD])  \r\n",
        "      \r\n",
        "        #Feature 1,8 (Topmost of stack): \r\n",
        "        if len(stack) == 0:\r\n",
        "            word_features.append(self.vocab_words[PAD]) \r\n",
        "            tag_features.append(self.vocab_tags[PAD])  \r\n",
        "        else:\r\n",
        "            word_features.append(words[stack[-1]])    \r\n",
        "            tag_features.append(tags[stack[-1]])      \r\n",
        "        \r\n",
        "        #Feature 1, 9 (Second topmost of stack):\r\n",
        "        if len(stack) <= 1:\r\n",
        "            word_features.append(self.vocab_words[PAD])\r\n",
        "            tag_features.append(self.vocab_tags[PAD]) \r\n",
        "        else:\r\n",
        "            word_features.append(words[stack[-2]])    \r\n",
        "            tag_features.append(tags[stack[-2]])              \r\n",
        "      \r\n",
        "        ####Above is Baseline features###########\r\n",
        "        if self.use_unigram:\r\n",
        "        ####Below is Unigram Features############\r\n",
        "            #left&rightmost modifier of topmost word on stack\r\n",
        "            if len(stack) != 0 and stack[-1] in head:\r\n",
        "                word_features.append(words[head[head.index(stack[-1])]])#s0Lw \r\n",
        "                tag_features.append(tags[head[head.index(stack[-1])]])#s0Lp\r\n",
        "                label_features.append(labler.predict(words[head[head.index(stack[-1])]], words[stack[-1]], \r\n",
        "                                                      tags[head[head.index(stack[-1])]], tags[stack[-1]]))\r\n",
        "                head.reverse() #Check from right to left\r\n",
        "                word_features.append(words[head[head.index(stack[-1])]])#s0Rw \r\n",
        "                tag_features.append(tags[head[head.index(stack[-1])]])#s0Rp\r\n",
        "                label_features.append(labler.predict(words[head[head.index(stack[-1])]], words[stack[-1]], \r\n",
        "                                                      tags[head[head.index(stack[-1])]], tags[stack[-1]]))\r\n",
        "                head.reverse()\r\n",
        "            else:\r\n",
        "                word_features.append(self.vocab_words[PAD])\r\n",
        "                word_features.append(self.vocab_words[PAD])\r\n",
        "                tag_features.append(self.vocab_tags[PAD])\r\n",
        "                tag_features.append(self.vocab_tags[PAD])\r\n",
        "                label_features.append(self.vocab_labels[PAD])\r\n",
        "                label_features.append(self.vocab_labels[PAD])\r\n",
        "            #left&rightmost modifier of second topmost word on stack (topmost word of queue in paper)\r\n",
        "            if len(stack) >= 2 and stack[-2] in head:\r\n",
        "                word_features.append(words[head[head.index(stack[-2])]])#s0Lw \r\n",
        "                tag_features.append(tags[head[head.index(stack[-2])]])#s0Lp\r\n",
        "                label_features.append(labler.predict(words[head[head.index(stack[-2])]], words[stack[-2]], \r\n",
        "                                                      tags[head[head.index(stack[-2])]], tags[stack[-2]]))\r\n",
        "                head.reverse() #Check from right to left\r\n",
        "                word_features.append(words[head[head.index(stack[-2])]])#s0Rw \r\n",
        "                tag_features.append(tags[head[head.index(stack[-2])]])#s0Rp\r\n",
        "                label_features.append(labler.predict(words[head[head.index(stack[-2])]], words[stack[-2]], \r\n",
        "                                                      tags[head[head.index(stack[-2])]], tags[stack[-2]]))\r\n",
        "                head.reverse()\r\n",
        "            else:\r\n",
        "                word_features.append(self.vocab_words[PAD])\r\n",
        "                word_features.append(self.vocab_words[PAD])\r\n",
        "                tag_features.append(self.vocab_tags[PAD])\r\n",
        "                tag_features.append(self.vocab_tags[PAD])\r\n",
        "                label_features.append(self.vocab_labels[PAD])\r\n",
        "                label_features.append(self.vocab_labels[PAD])        \r\n",
        "        ####Above is Unigram Features############\r\n",
        "        if self.use_third_order:\r\n",
        "        ####Below is Third order relation########\r\n",
        "            if len(stack) != 0 and stack[-1] in head and len([i for i, head_pos in enumerate(head) if head_pos == stack[-1]]) > 1:\r\n",
        "                second_leftmost_modifier = [i for i, head_pos in enumerate(head) if head_pos == stack[-1]][1] #Last indexation gives the SECOND leftmost word in heads\r\n",
        "                word_features.append(words[head[second_leftmost_modifier]])#s0Lw \r\n",
        "                tag_features.append(tags[head[second_leftmost_modifier]])#s0Lp\r\n",
        "                label_features.append(labler.predict(words[head[second_leftmost_modifier]], words[stack[-1]], \r\n",
        "                                                      tags[head[second_leftmost_modifier]], tags[stack[-1]]))\r\n",
        "                head.reverse() #Check from right to left\r\n",
        "                second_rightmost_modifier = [i for i, head_pos in enumerate(head) if head_pos == stack[-1]][1] #Last indexation gives the SECOND rightmost word in heads \r\n",
        "                word_features.append(words[head[second_rightmost_modifier]])#s0Rw \r\n",
        "                tag_features.append(tags[head[second_rightmost_modifier]])#s0Rp\r\n",
        "                label_features.append(labler.predict(words[head[second_rightmost_modifier]], words[stack[-1]], \r\n",
        "                                                      tags[head[second_rightmost_modifier]], tags[stack[-1]]))\r\n",
        "                head.reverse()\r\n",
        "            else:\r\n",
        "                word_features.append(self.vocab_words[PAD])\r\n",
        "                word_features.append(self.vocab_words[PAD])\r\n",
        "                tag_features.append(self.vocab_tags[PAD])\r\n",
        "                tag_features.append(self.vocab_tags[PAD])\r\n",
        "                label_features.append(self.vocab_labels[PAD])\r\n",
        "                label_features.append(self.vocab_labels[PAD])\r\n",
        "            #left&rightmost modifier of second topmost word on stack (topmost word of queue in paper)\r\n",
        "            if len(stack) >= 2 and stack[-2] in head and len([i for i, head_pos in enumerate(head) if head_pos == stack[-2]]) > 1:\r\n",
        "                second_leftmost_modifier = [i for i, head_pos in enumerate(head) if head_pos == stack[-2]][1] #Last indexation gives the SECOND leftmost word in heads\r\n",
        "                word_features.append(words[head[second_leftmost_modifier]])#s0Lw \r\n",
        "                tag_features.append(tags[head[second_leftmost_modifier]])#s0Lp\r\n",
        "                label_features.append(labler.predict(words[head[second_leftmost_modifier]], words[stack[-2]], \r\n",
        "                                                      tags[head[second_leftmost_modifier]], tags[stack[-2]]))\r\n",
        "                head.reverse() #Check from right to left\r\n",
        "                second_rightmost_modifier = [i for i, head_pos in enumerate(head) if head_pos == stack[-2]][1] #Last indexation gives the SECOND rightmost word in heads\r\n",
        "                word_features.append(words[head[second_rightmost_modifier]])#s0Rw \r\n",
        "                tag_features.append(tags[head[second_rightmost_modifier]])#s0Rp\r\n",
        "                label_features.append(labler.predict(words[head[second_rightmost_modifier]], words[stack[-2]], \r\n",
        "                                                      tags[head[second_rightmost_modifier]], tags[stack[-2]]))\r\n",
        "                head.reverse()\r\n",
        "            else:\r\n",
        "                word_features.append(self.vocab_words[PAD])\r\n",
        "                word_features.append(self.vocab_words[PAD])\r\n",
        "                tag_features.append(self.vocab_tags[PAD])\r\n",
        "                tag_features.append(self.vocab_tags[PAD])\r\n",
        "                label_features.append(self.vocab_labels[PAD])\r\n",
        "                label_features.append(self.vocab_labels[PAD])  \r\n",
        "        ####Above is Third order relation########\r\n",
        "        ####Below is Valency#####################\r\n",
        "        valency_features = []\r\n",
        "        if self.use_valency :\r\n",
        "            if len(stack)>1:\r\n",
        "                top_left_valency = sum(1 for k in head[:i] if k==stack[-1])\r\n",
        "                top_right_valency = sum(1 for k in head[i:] if k==stack[-1])\r\n",
        "                second_left_valency = sum(1 for k in head[:i] if k==stack[-2])\r\n",
        "                second_right_valency = sum(1 for k in head[i:] if k==stack[-2])\r\n",
        "                valency_features.append(top_left_valency)\r\n",
        "                valency_features.append(top_right_valency)\r\n",
        "                valency_features.append(second_left_valency)\r\n",
        "                valency_features.append(second_right_valency)\r\n",
        "            elif len(stack)==1:\r\n",
        "                top_left_valency = sum(1 for k in head[:i] if k==stack[-1])\r\n",
        "                top_right_valency = sum(1 for k in head[i:] if k==stack[-1])\r\n",
        "                valency_features.append(top_left_valency)\r\n",
        "                valency_features.append(top_right_valency)\r\n",
        "                valency_features.append(0)\r\n",
        "                valency_features.append(0)\r\n",
        "            else:\r\n",
        "                valency_features.append(0)\r\n",
        "                valency_features.append(0)\r\n",
        "                valency_features.append(0)\r\n",
        "                valency_features.append(0)          \r\n",
        "        ####Above is Valency#####################\r\n",
        "        featurized_features = word_features + tag_features + label_features + valency_features\r\n",
        "\r\n",
        "        return torch.LongTensor(featurized_features)\r\n",
        "    def predict(self, words, tags, want_print = False):\r\n",
        "        word_ids = []\r\n",
        "        tag_ids = []\r\n",
        "        for word in words:\r\n",
        "            word_ids.append(self.vocab_words[word] if word in self.vocab_words else self.vocab_words[UNK])\r\n",
        "        for tag in tags:\r\n",
        "            tag_ids.append(self.vocab_tags[tag])\r\n",
        "        config = self.initial_config(len(words))\r\n",
        "        while not self.is_final_config(config):\r\n",
        "            model_features = self.featurize(word_ids, tag_ids, config, want_print = want_print)\r\n",
        "            if (want_print):\r\n",
        "                print(\"In predict: \" + str(model_features.shape))\r\n",
        "            pred = self.fw_model.forward(model_features, want_print = want_print)\r\n",
        "            try:\r\n",
        "                valid_moves = self.valid_moves(config)\r\n",
        "            except:\r\n",
        "                print(\"pred: \" + str(pred))\r\n",
        "                print(\"model_features: \" + str(model_features))\r\n",
        "                print(\"model_features: \" + str(model_features.shape))\r\n",
        "\r\n",
        "            best_move_score = -math.inf\r\n",
        "            best_move = None\r\n",
        "            for move in valid_moves:\r\n",
        "                if pred[0,move].item() > best_move_score:\r\n",
        "                    best_move = move\r\n",
        "                    best_move_score = pred[0,move].item()\r\n",
        "            if want_print:\r\n",
        "                print(config)\r\n",
        "            config = self.next_config(config, best_move)\r\n",
        "        return config[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--X3SGrMUwAK"
      },
      "source": [
        "def parse_training_examples(vocab_words, vocab_tags, inverse_vocab_words, \r\n",
        "                            inverse_vocab_tags, gold_data, parser, \r\n",
        "                            feature_k, batch_size=100):\r\n",
        "  batch_i = 0\r\n",
        "  batch_x = torch.zeros((batch_size, feature_k), dtype=int)\r\n",
        "  batch_y = torch.zeros(batch_size, dtype=int)\r\n",
        "  for sentence in gold_data:\r\n",
        "      word_ids = []\r\n",
        "      tag_ids = []\r\n",
        "      heads = []\r\n",
        "      for trip in sentence:\r\n",
        "          word_ids.append(vocab_words[trip[0]])\r\n",
        "          tag_ids.append(vocab_tags[trip[1]])\r\n",
        "          heads.append(trip[2])\r\n",
        "      \r\n",
        "      for config, move in tuple(oracle_moves(heads))[1:]:\r\n",
        "          if batch_i > batch_size - 1:\r\n",
        "              yield batch_x, batch_y\r\n",
        "              batch_x = torch.zeros((batch_size, feature_k), dtype=int)\r\n",
        "              batch_y = torch.zeros(batch_size, dtype=int)\r\n",
        "              batch_i = 0\r\n",
        "          batch_x[batch_i,:] = parser.featurize(word_ids, tag_ids, config)\r\n",
        "          batch_y[batch_i] = move \r\n",
        "          batch_i += 1\r\n",
        "  if not batch_i == 0:\r\n",
        "      yield batch_x[0:batch_i,:], batch_y[0:batch_i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-DPPqLWU5GN"
      },
      "source": [
        "def parse_train_fixed_window(train_data, n_epochs=2, batch_size=100, lr=1e-2, \r\n",
        "        embedding_specs=[(3, len(vocab_words), 50), (3, len(vocab_tags), 10)], \r\n",
        "        feature_k = 6, use_distance = False, use_third_order = False, \r\n",
        "        use_unigram = False, use_valency = False):\r\n",
        "    vocab_words, vocab_tags, vocab_labels, inverse_vocab_words, inverse_vocab_tags, inverse_vocab_labels = make_vocabs(label_train_data)\r\n",
        "    parser = FixedWindowParser(vocab_words, vocab_tags, embedding_specs=embedding_specs, \r\n",
        "                               use_distance=use_distance, use_unigram=use_unigram, \r\n",
        "                               use_third_order=use_third_order, use_valency=use_valency)\r\n",
        "    optimizer = optim.Adam(parser.fw_model.parameters(), lr=lr)\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    for ep in range(n_epochs):\r\n",
        "        parser.fw_model.train()\r\n",
        "        n_batch = 0\r\n",
        "        for bx, by in parse_training_examples(vocab_words, vocab_tags, inverse_vocab_words, inverse_vocab_tags, train_data, parser, feature_k, batch_size=batch_size):\r\n",
        "            bx = bx.to(device)\r\n",
        "            by = by.to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            output = parser.fw_model.forward(bx)\r\n",
        "            loss = F.cross_entropy(output, by)\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            n_batch += 1\r\n",
        "        parser.fw_model.eval()\r\n",
        "    \r\n",
        "    return parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmYs6F9iSeco"
      },
      "source": [
        "def mean_list(test_list):\r\n",
        "    mean = sum(test_list) / len(test_list) \r\n",
        "    return str(mean)\r\n",
        "\r\n",
        "def var_list(test_list):\r\n",
        "    mean = sum(test_list) / len(test_list) \r\n",
        "    res = sum((i - mean) ** 2 for i in test_list) / len(test_list)\r\n",
        "    return str(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B7x0TZNVRMq"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wOO_HmWbaYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c95477f-e34f-4214-a196-4934105b2223"
      },
      "source": [
        "tagger = tag_train_fixed_window(tag_train_data, n_epochs=2)\r\n",
        "print(\"Pure Tagger Baseline: \" + '{:.4f}'.format(accuracy(tagger, tag_dev_data)))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pure Tagger Baseline: 0.8766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeTUM8W3eX9F"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Y2qCEMdtut"
      },
      "source": [
        "## Baseline only\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1I88NXYfMPy",
        "outputId": "2dd635d4-7bdb-4040-c7ad-607c0e23f838"
      },
      "source": [
        "uas_list_baseline = []\r\n",
        "for _ in range(10):\r\n",
        "    vocab_words, vocab_tags, vocab_labels, inverse_vocab_words, inverse_vocab_tags, inverse_vocab_labels = make_vocabs(label_train_data)\r\n",
        "    parser = parse_train_fixed_window(parse_train_data, n_epochs=2)\r\n",
        "    pipeline_valid_data = []\r\n",
        "    for sentence in list(parse_dev_data):\r\n",
        "        words = []\r\n",
        "        heads = []\r\n",
        "        for trip in sentence:\r\n",
        "            words.append(trip[0])\r\n",
        "            heads.append(trip[2])\r\n",
        "        pred_tags = tagger.predict(words)\r\n",
        "        valid_sentence = []\r\n",
        "        for i in range(len(words)):\r\n",
        "            valid_sentence.append((words[i], pred_tags[i], heads[i]))\r\n",
        "        pipeline_valid_data.append(valid_sentence)\r\n",
        "    uas_list_baseline.append(uas(parser, pipeline_valid_data))\r\n",
        "\r\n",
        "print(\"Pipeline Baseline Mean Accuracy: \" + mean_list(uas_list_baseline)+ \", and Variance: \" + var_list(uas_list_baseline))\r\n",
        "print(\"All values: \" + str(uas_list_baseline))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline Baseline Mean Accuracy: 0.5578141040827609, and Variance: 8.139265540956428e-05\n",
            "All values: [0.5674010897891495, 0.5598989181078733, 0.5660585959093422, 0.5552396746426597, 0.5556345257837795, 0.5503435204927742, 0.5670062386480297, 0.5618731738134723, 0.5587933349127379, 0.5358919687277897]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhWRZXdCdoIb"
      },
      "source": [
        "## Baseline + Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWcwRB3GdujO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5def13f1-cf60-420c-c2e4-a0854a56d311"
      },
      "source": [
        "uas_list_distance = []\r\n",
        "for _ in range(10):\r\n",
        "    parser = parse_train_fixed_window(parse_train_data, n_epochs=3, use_distance = True)\r\n",
        "    pipeline_valid_data = []\r\n",
        "    for sentence in list(parse_dev_data):\r\n",
        "        words = []\r\n",
        "        heads = []\r\n",
        "        for trip in sentence:\r\n",
        "            words.append(trip[0])\r\n",
        "            heads.append(trip[2])\r\n",
        "        pred_tags = tagger.predict(words)\r\n",
        "        valid_sentence = []\r\n",
        "        for i in range(len(words)):\r\n",
        "            valid_sentence.append((words[i], pred_tags[i], heads[i]))\r\n",
        "        pipeline_valid_data.append(valid_sentence)\r\n",
        "    uas_list_distance.append(uas(parser, pipeline_valid_data))\r\n",
        "\r\n",
        "print(\"Pipeline Baseline Mean Accuracy: \" + mean_list(uas_list_distance)+ \", and Variance: \" + var_list(uas_list_distance))\r\n",
        "print(\"All values: \" + str(uas_list_distance))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline Baseline Mean Accuracy: 0.5667219458264234, and Variance: 6.317892654982375e-05\n",
            "All values: [0.5628208165521599, 0.5654268340835505, 0.5691384348100766, 0.5809049988154465, 0.5756929637526652, 0.5715075416567954, 0.5640843402037432, 0.5579246624022743, 0.5677959409302693, 0.5519229250572534]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okB3_TLZKfuU"
      },
      "source": [
        "Pipeline Baseline Mean Accuracy: 0.6297558453952601, and Variance: 0.00011954375634655416\r\n",
        "All values: [0.6171862573564498, 0.6375854938762526, 0.6330920947987911, 0.6221965961507874, 0.6267297598218546, 0.6223556545252108, 0.6351200890726897, 0.6560362653093685, 0.6285986957213298, 0.6186575473198664]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC6wWObrdkhI"
      },
      "source": [
        "## Baseline + Unigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ketKKbv3eAdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e2a468f-0cf4-416c-c6ff-3f8a6f8bfeb7"
      },
      "source": [
        "uas_list_unigram = []\r\n",
        "for _ in range(10):\r\n",
        "    embedding_specs = [(7, len(vocab_words), 50), (7, len(vocab_tags), 10), (4, len(vocab_labels), 10)]\r\n",
        "    feature_k = 18\r\n",
        "    parser = parse_train_fixed_window(parse_train_data, n_epochs=2, embedding_specs=embedding_specs, feature_k=feature_k, use_unigram = True)\r\n",
        "    pipeline_valid_data = []\r\n",
        "    for sentence in list(parse_dev_data):\r\n",
        "        words = []\r\n",
        "        heads = []\r\n",
        "        for trip in sentence:\r\n",
        "            words.append(trip[0])\r\n",
        "            heads.append(trip[2])\r\n",
        "        pred_tags = tagger.predict(words)\r\n",
        "        valid_sentence = []\r\n",
        "        for i in range(len(words)):\r\n",
        "            valid_sentence.append((words[i], pred_tags[i], heads[i]))\r\n",
        "        pipeline_valid_data.append(valid_sentence)\r\n",
        "\r\n",
        "    uas_list_unigram.append(uas(parser, pipeline_valid_data))\r\n",
        "\r\n",
        "print(\"Pipeline Baseline Mean Accuracy: \" + mean_list(uas_list_unigram)+ \", and Variance: \" + var_list(uas_list_unigram))\r\n",
        "print(\"All values: \" + str(uas_list_unigram))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline Baseline Mean Accuracy: 0.5775566611387506, and Variance: 2.2258840510628266e-05\n",
            "All values: [0.5794835347074153, 0.5818526415541341, 0.5752981126115454, 0.5766406064913527, 0.5843007186290768, 0.5786148621969518, 0.576087814893785, 0.5657427149964463, 0.5775092790018164, 0.580036326304983]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYOkDWw5dgm-"
      },
      "source": [
        "## Baseline + Third Order\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cxSKm8Hg9kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c20cd9-f781-4647-c977-c3d45e7f8d24"
      },
      "source": [
        "uas_list_third_order = []\r\n",
        "for _ in range(10):\r\n",
        "    embedding_specs = [(7, len(vocab_words), 50), (7, len(vocab_tags), 10), (4, len(vocab_labels), 12)]\r\n",
        "    feature_k = 18\r\n",
        "    parser = parse_train_fixed_window(parse_train_data, n_epochs=2, embedding_specs=embedding_specs, feature_k=feature_k, use_third_order = True)\r\n",
        "    pipeline_valid_data = []\r\n",
        "    for sentence in list(parse_dev_data):\r\n",
        "        words = []\r\n",
        "        heads = []\r\n",
        "        for trip in sentence:\r\n",
        "            words.append(trip[0])\r\n",
        "            heads.append(trip[2])\r\n",
        "        pred_tags = tagger.predict(words)\r\n",
        "        valid_sentence = []\r\n",
        "        for i in range(len(words)):\r\n",
        "            valid_sentence.append((words[i], pred_tags[i], heads[i]))\r\n",
        "        pipeline_valid_data.append(valid_sentence)\r\n",
        "\r\n",
        "    uas_list_third_order.append(uas(parser, pipeline_valid_data))\r\n",
        "\r\n",
        "print(\"Pipeline Baseline Mean Accuracy: \" + mean_list(uas_list_third_order)+ \", and Variance: \" + var_list(uas_list_third_order))\r\n",
        "print(\"All values: \" + str(uas_list_third_order))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline Baseline Mean Accuracy: 0.5725736397378188, and Variance: 4.9607559499464636e-05\n",
            "All values: [0.5671641791044776, 0.5640053699755192, 0.5782989812840559, 0.5817736713259102, 0.5764036958066808, 0.5747453210139777, 0.5588723051409619, 0.5690594645818526, 0.5763247255784569, 0.5790886835662955]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0-5vHlda7a"
      },
      "source": [
        "## Baseline + Valency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1k_6I4ewgRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b750c4f-c78a-4322-92c0-595cafcd84c3"
      },
      "source": [
        "uas_list_valency = []\r\n",
        "for _ in range(10):\r\n",
        "    feature_k = 10\r\n",
        "    parser = parse_train_fixed_window(parse_train_data, n_epochs=2, feature_k=feature_k, use_valency = True)\r\n",
        "    pipeline_valid_data = []\r\n",
        "    for sentence in list(parse_dev_data):\r\n",
        "        words = []\r\n",
        "        heads = []\r\n",
        "        for trip in sentence:\r\n",
        "            words.append(trip[0])\r\n",
        "            heads.append(trip[2])\r\n",
        "        pred_tags = tagger.predict(words)\r\n",
        "        valid_sentence = []\r\n",
        "        for i in range(len(words)):\r\n",
        "            valid_sentence.append((words[i], pred_tags[i], heads[i]))\r\n",
        "        pipeline_valid_data.append(valid_sentence)\r\n",
        "\r\n",
        "    uas_list_valency.append(uas(parser, pipeline_valid_data))\r\n",
        "\r\n",
        "print(\"Pipeline Baseline Mean Accuracy: \" + mean_list(uas_list_valency)+ \", and Variance: \" + var_list(uas_list_valency))\r\n",
        "print(\"All values: \" + str(uas_list_valency))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline Baseline Mean Accuracy: 0.578859669904446, and Variance: 5.149366514773534e-05\n",
            "All values: [0.5810629392718945, 0.5720603332543631, 0.5891968727789624, 0.5807470583589986, 0.5921977414514727, 0.5707968096027798, 0.5801152965332069, 0.5749032614704257, 0.5692174050383005, 0.5782989812840559]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3j3qDnxdR6c"
      },
      "source": [
        "## Baseline + all features except distance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIwf_BrMc1wq",
        "outputId": "8487e309-e290-4c7f-cfa2-0b2702214334"
      },
      "source": [
        "uas_list_all = []\r\n",
        "for _ in range(10):\r\n",
        "    print(\"Iteration\", _+1)\r\n",
        "    embedding_specs = [(11, len(vocab_words), 50), (11, len(vocab_tags), 10), (8, len(vocab_labels), 12)]\r\n",
        "    feature_k = 34 \r\n",
        "    parser = parse_train_fixed_window(parse_train_data, n_epochs=2, \r\n",
        "                                      embedding_specs=embedding_specs, \r\n",
        "                                      feature_k=feature_k,\r\n",
        "                                      use_distance = False, \r\n",
        "                                      use_unigram = True, \r\n",
        "                                      use_third_order = True, \r\n",
        "                                      use_valency = True)\r\n",
        "    pipeline_valid_data = []\r\n",
        "    for sentence in list(parse_dev_data):\r\n",
        "        words = []\r\n",
        "        heads = []\r\n",
        "        for trip in sentence:\r\n",
        "            words.append(trip[0])\r\n",
        "            heads.append(trip[2])\r\n",
        "        pred_tags = tagger.predict(words)\r\n",
        "        valid_sentence = []\r\n",
        "        for i in range(len(words)):\r\n",
        "            valid_sentence.append((words[i], pred_tags[i], heads[i]))\r\n",
        "        pipeline_valid_data.append(valid_sentence)\r\n",
        "\r\n",
        "    uas_list_all.append(uas(parser, pipeline_valid_data))\r\n",
        "\r\n",
        "print(\"Pipeline Baseline + All features Mean Accuracy: \" + mean_list(uas_list_all)+ \", and Variance: \" + var_list(uas_list_all))\r\n",
        "print(\"All values: \" + str(uas_list_all))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "Iteration 10\n",
            "Pipeline Baseline + All features Mean Accuracy: 0.5712548369264787, and Variance: 2.7600353570597147e-05\n",
            "All values: [0.5731659164494985, 0.5647161020295348, 0.5730869462212745, 0.5612414119876806, 0.5736397378188423, 0.5715865118850193, 0.5732448866777226, 0.5795625049356392, 0.5760088446655611, 0.5662955065940141]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QXcznJxdIww"
      },
      "source": [
        "## Baseline + all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcKWc5QydExg",
        "outputId": "7ebe6c75-6ff3-47b8-ce65-4f3650578b02"
      },
      "source": [
        "uas_list_all = []\r\n",
        "for _ in range(10):\r\n",
        "    print(\"Iteration\", _+1)\r\n",
        "    embedding_specs = [(11, len(vocab_words), 50), (11, len(vocab_tags), 10), (8, len(vocab_labels), 12)]\r\n",
        "    feature_k = 34 \r\n",
        "    parser = parse_train_fixed_window(parse_train_data, n_epochs=2, \r\n",
        "                                      embedding_specs=embedding_specs, \r\n",
        "                                      feature_k=feature_k,\r\n",
        "                                      use_distance = True, \r\n",
        "                                      use_unigram = True, \r\n",
        "                                      use_third_order = True, \r\n",
        "                                      use_valency = True)\r\n",
        "    pipeline_valid_data = []\r\n",
        "    for sentence in list(parse_dev_data):\r\n",
        "        words = []\r\n",
        "        heads = []\r\n",
        "        for trip in sentence:\r\n",
        "            words.append(trip[0])\r\n",
        "            heads.append(trip[2])\r\n",
        "        pred_tags = tagger.predict(words)\r\n",
        "        valid_sentence = []\r\n",
        "        for i in range(len(words)):\r\n",
        "            valid_sentence.append((words[i], pred_tags[i], heads[i]))\r\n",
        "        pipeline_valid_data.append(valid_sentence)\r\n",
        "\r\n",
        "    uas_list_all.append(uas(parser, pipeline_valid_data))\r\n",
        "\r\n",
        "print(\"Pipeline Baseline + All features Mean Accuracy: \" + mean_list(uas_list_all)+ \", and Variance: \" + var_list(uas_list_all))\r\n",
        "print(\"All values: \" + str(uas_list_all))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "Iteration 10\n",
            "Pipeline Baseline + All features Mean Accuracy: 0.5787333175392877, and Variance: 9.86862810179304e-06\n",
            "All values: [0.5793255942509674, 0.5771933980889205, 0.5727710653083787, 0.5832741056621653, 0.5771144278606966, 0.5747453210139777, 0.5823264629234779, 0.5792466240227434, 0.5812208797283425, 0.5801152965332069]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}